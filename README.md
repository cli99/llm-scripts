# llm-scripts

- [attention](attn)
  - [flashattn2.6.2_vs_flashinfer0.1.6_h100_float16](attn/flashattn2.6.2_vs_flashinfer0.1.6_h100_float16)